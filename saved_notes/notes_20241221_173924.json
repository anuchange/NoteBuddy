{
    "video_id": "qrsNX1Rwle0",
    "notes": "Okay this part I understood",
    "summary": "**1. Concise Summary of the Main Points**\n\nThe speaker introduces Transformers and compares them to Recurrent Neural Networks (RNNs), which were previously dominant in NLP and some vision applications. The discussion focuses on the limitations of RNNs and motivates the use of Transformers. The primary challenge with RNNs is that they process input sequentially, even when the entire input is available at once. This sequential processing reduces computational efficiency. The speaker aims to overcome this issue by obtaining contextual representations without relying on sequential computation.\n\n**2. Key Takeaways**\n\n- Transformers have replaced RNNs in many NLP and vision applications.\n- RNNs process input sequentially, which reduces computational efficiency.\n- Contextual representations are important for understanding the meaning of words in a sentence.\n- The speaker aims to develop a model that can compute contextual representations without relying on sequential processing.\n\n**3. Important Concepts Discussed**\n\n- Recurrent Neural Networks (RNNs): Neural networks that process input sequentially, maintaining a hidden state from time step to time step.\n- Contextual Representations: Word embeddings that take into account the context of a word within a sentence.\n- Transformers: A type of model that can compute contextual representations without relying on sequential processing.\n- Attention Mechanism: A mechanism that allows a model to focus on specific parts of the input when computing outputs. In this transcript, the speaker discusses the possibility of computing attention weights in parallel, which can improve computational efficiency.",
    "timestamps": [
        {
            "time": 0.33,
            "text": "[Music]"
        },
        {
            "time": 15.599,
            "text": "so we'll do uh an introduction to"
        },
        {
            "time": 18.96,
            "text": "Transformers and compare them largely"
        },
        {
            "time": 21.279,
            "text": "with uh the recurrent neural networks"
        },
        {
            "time": 23.199,
            "text": "which was the previous uh dominant set"
        },
        {
            "time": 25.92,
            "text": "of models in uh various applications in"
        },
        {
            "time": 29.0,
            "text": "NLP as well as a few uh Vision"
        },
        {
            "time": 31.16,
            "text": "applications like image captioning and"
        },
        {
            "time": 32.88,
            "text": "so on right uh and now a lot of them"
        },
        {
            "time": 34.96,
            "text": "have been replaced by Transformers so"
        },
        {
            "time": 37.239,
            "text": "that's what we're going to uh talk about"
        },
        {
            "time": 39.6,
            "text": "today so to uh motivate right what what"
        },
        {
            "time": 43.239,
            "text": "is the content today going to look like"
        },
        {
            "time": 45.12,
            "text": "right so we have seen three types of"
        },
        {
            "time": 46.64,
            "text": "architectures one was the feed forward"
        },
        {
            "time": 48.32,
            "text": "neural networks then the convolutional"
        },
        {
            "time": 50.48,
            "text": "neural networks and then recurrent"
        },
        {
            "time": 51.92,
            "text": "neural networks right and in each of"
        },
        {
            "time": 53.8,
            "text": "this we saw the building block and then"
        },
        {
            "time": 56.399,
            "text": "once we knew understood the building"
        },
        {
            "time": 57.879,
            "text": "block properly we could envision deep"
        },
        {
            "time": 60.68,
            "text": "wide networks right so in the case of"
        },
        {
            "time": 62.44,
            "text": "feed forward neural networks the"
        },
        {
            "time": 64.239,
            "text": "building block was essentially this"
        },
        {
            "time": 66.56,
            "text": "sigmoid neuron right or any nonlinear"
        },
        {
            "time": 69.32,
            "text": "neuron which used to take a bunch of"
        },
        {
            "time": 72.0,
            "text": "inputs do a weighted aggregation and"
        },
        {
            "time": 75.36,
            "text": "then pass it through a nonlinearity"
        },
        {
            "time": 77.52,
            "text": "right so that was the basic building"
        },
        {
            "time": 79.32,
            "text": "block and then we had many of these"
        },
        {
            "time": 80.92,
            "text": "connected in a layer and across layers"
        },
        {
            "time": 83.159,
            "text": "to get a deep in wide neural network"
        },
        {
            "time": 84.799,
            "text": "right similarly in the case of"
        },
        {
            "time": 86.079,
            "text": "convolutional neural networks the basic"
        },
        {
            "time": 87.92,
            "text": "building block was a convolution"
        },
        {
            "time": 89.24,
            "text": "operation and maybe even the max pooling"
        },
        {
            "time": 91.159,
            "text": "operation then in the case of recurrent"
        },
        {
            "time": 93.159,
            "text": "neural networks the basic building block"
        },
        {
            "time": 95.32,
            "text": "was this recurrent uh equation where you"
        },
        {
            "time": 98.52,
            "text": "could compute HT which was a state at"
        },
        {
            "time": 100.88,
            "text": "time t as a function of uh HT minus one"
        },
        {
            "time": 106.64,
            "text": "and the input at that time step right so"
        },
        {
            "time": 109.159,
            "text": "this is what the basic BL building block"
        },
        {
            "time": 111.6,
            "text": "was and then we saw the attention based"
        },
        {
            "time": 114.64,
            "text": "uh recurrent neural network where we had"
        },
        {
            "time": 116.759,
            "text": "the attention uh function which was like"
        },
        {
            "time": 119.68,
            "text": "a a basic building block right so I'm"
        },
        {
            "time": 121.039,
            "text": "calling these basic building blocks"
        },
        {
            "time": 122.32,
            "text": "because if you understand these then"
        },
        {
            "time": 124.079,
            "text": "it's not very difficult to understand"
        },
        {
            "time": 125.92,
            "text": "the full Network right so similarly"
        },
        {
            "time": 128.2,
            "text": "today we'll focus on the basic building"
        },
        {
            "time": 130.319,
            "text": "blocks of Transformers which is larly"
        },
        {
            "time": 132.56,
            "text": "the attention uh the self attention and"
        },
        {
            "time": 134.84,
            "text": "the cross attention uh layers that it"
        },
        {
            "time": 137.64,
            "text": "uses and then try to relate it to what"
        },
        {
            "time": 140.44,
            "text": "we have already seen in attention based"
        },
        {
            "time": 142.28,
            "text": "models in the context of uh recurrent"
        },
        {
            "time": 144.12,
            "text": "neural networks right so that's the idea"
        },
        {
            "time": 146.56,
            "text": "so with that let's uh zoom into uh the"
        },
        {
            "time": 149.4,
            "text": "RM based models and just see some"
        },
        {
            "time": 151.28,
            "text": "limitations of it right so one challenge"
        },
        {
            "time": 154.72,
            "text": "in RNN based models was that if I'm"
        },
        {
            "time": 157.599,
            "text": "looking at the use case of translation"
        },
        {
            "time": 160.12,
            "text": "right then my input is I am"
        },
        {
            "time": 165.08,
            "text": "going home and I want to produce its"
        },
        {
            "time": 168.599,
            "text": "translation in the target language right"
        },
        {
            "time": 171.879,
            "text": "now the input is given to me at one go"
        },
        {
            "time": 174.72,
            "text": "right the input is not like coming to me"
        },
        {
            "time": 176.72,
            "text": "one word at a time right I just tell the"
        },
        {
            "time": 178.319,
            "text": "machine hey this is the sentence that I"
        },
        {
            "time": 179.599,
            "text": "want to translate right but the"
        },
        {
            "time": 181.879,
            "text": "computation is not happening at one go"
        },
        {
            "time": 184.319,
            "text": "right so let's see what I mean by that"
        },
        {
            "time": 185.959,
            "text": "right so what happens here is that you"
        },
        {
            "time": 188.72,
            "text": "get the"
        },
        {
            "time": 191.239,
            "text": "uh you have the initialization Vector"
        },
        {
            "time": 194.08,
            "text": "right and then you have this word I and"
        },
        {
            "time": 197.239,
            "text": "then you pass it through the Nu neural"
        },
        {
            "time": 199.0,
            "text": "network so what you have here is say the"
        },
        {
            "time": 201.08,
            "text": "embedding of the word I right so let's"
        },
        {
            "time": 203.0,
            "text": "call this as X1 so this is the first"
        },
        {
            "time": 205.28,
            "text": "word so you have the embedding of that"
        },
        {
            "time": 207.04,
            "text": "and then what the output is H1 right"
        },
        {
            "time": 209.76,
            "text": "what you're Computing here is H1 which"
        },
        {
            "time": 212.799,
            "text": "is a function of h0 and X1 right and now"
        },
        {
            "time": 219.879,
            "text": "once I've done that this H1 then becomes"
        },
        {
            "time": 222.599,
            "text": "an input to the next computation because"
        },
        {
            "time": 225.36,
            "text": "H2 will be computed as H1 comma X2 right"
        },
        {
            "time": 229.879,
            "text": "so let's see this in the figure now so I"
        },
        {
            "time": 232.0,
            "text": "have H1 I compute H1 then I get uh the"
        },
        {
            "time": 237.12,
            "text": "next input which is enjoyed right it has"
        },
        {
            "time": 239.239,
            "text": "some another another sentence so I think"
        },
        {
            "time": 240.64,
            "text": "it's I enjoyed the movie Transformers so"
        },
        {
            "time": 243.2,
            "text": "I got H1 and now I use that and H1 right"
        },
        {
            "time": 247.239,
            "text": "so essentially I'm using this function"
        },
        {
            "time": 249.239,
            "text": "which is H1 comma X2 to compute this"
        },
        {
            "time": 252.48,
            "text": "hidden representation or this yellow"
        },
        {
            "time": 254.2,
            "text": "representation that you see here right"
        },
        {
            "time": 256.519,
            "text": "so what is happening here is that my"
        },
        {
            "time": 257.88,
            "text": "computations are happening sequentially"
        },
        {
            "time": 260.72,
            "text": "although my input was given to me at one"
        },
        {
            "time": 263.24,
            "text": "go I had the entire sentence at one go"
        },
        {
            "time": 265.84,
            "text": "and now let's just get rid of some of"
        },
        {
            "time": 267.56,
            "text": "these annotations and just see the whole"
        },
        {
            "time": 269.32,
            "text": "thing right so I had the entire sentence"
        },
        {
            "time": 271.52,
            "text": "and I'm still Computing it one step at a"
        },
        {
            "time": 273.4,
            "text": "time right I computed H H1 then H2 then"
        },
        {
            "time": 276.52,
            "text": "I'm Computing H3 then H4 and so on right"
        },
        {
            "time": 278.919,
            "text": "so I'm just doing all of this one uh"
        },
        {
            "time": 281.68,
            "text": "step at a time although this entire"
        },
        {
            "time": 284.08,
            "text": "sentence was available to me right at"
        },
        {
            "time": 286.72,
            "text": "the beginning right so now the question"
        },
        {
            "time": 289.16,
            "text": "is what's the benefit of what is"
        },
        {
            "time": 291.4,
            "text": "happening here right so why am I"
        },
        {
            "time": 292.919,
            "text": "Computing H2 only after I have looked at"
        },
        {
            "time": 295.919,
            "text": "H1 right so the reason for that is that"
        },
        {
            "time": 298.52,
            "text": "I know that for every word I already"
        },
        {
            "time": 301.479,
            "text": "have the word embedding right so that's"
        },
        {
            "time": 302.88,
            "text": "what my input is and this word embedding"
        },
        {
            "time": 304.44,
            "text": "could be computed from your uh favorite"
        },
        {
            "time": 306.6,
            "text": "algorithm like word to or fastex or"
        },
        {
            "time": 309.0,
            "text": "glove embedding or it could just be"
        },
        {
            "time": 311.12,
            "text": "randomly initialized it could just be a"
        },
        {
            "time": 312.56,
            "text": "learnable parameter in the network right"
        },
        {
            "time": 315.68,
            "text": "uh but that is uh the word that is the"
        },
        {
            "time": 318.36,
            "text": "embedding of the word computed from a"
        },
        {
            "time": 320.52,
            "text": "corpus but I want to know the embedding"
        },
        {
            "time": 323.36,
            "text": "of this word in the context of this"
        },
        {
            "time": 325.36,
            "text": "sentence so that's what these Yellow"
        },
        {
            "time": 326.72,
            "text": "Boxes are allowing me to compute because"
        },
        {
            "time": 328.84,
            "text": "they are taking input from the rest of"
        },
        {
            "time": 330.639,
            "text": "the sentence right now you might ask hey"
        },
        {
            "time": 333.0,
            "text": "I'm only taking input from one side"
        },
        {
            "time": 334.639,
            "text": "right I am not really considering"
        },
        {
            "time": 335.919,
            "text": "Transformers when I'm Computing the"
        },
        {
            "time": 338.16,
            "text": "yellow box for uh movie right but there"
        },
        {
            "time": 341.72,
            "text": "are also known as something known as"
        },
        {
            "time": 343.039,
            "text": "bidirectional RNN or bidirectional lstms"
        },
        {
            "time": 345.759,
            "text": "where you start from right to left right"
        },
        {
            "time": 347.88,
            "text": "so this is how the computation proceeds"
        },
        {
            "time": 350.24,
            "text": "for all practical purposes you could"
        },
        {
            "time": 351.8,
            "text": "think of I have one sentence I enjoyed"
        },
        {
            "time": 354.039,
            "text": "the movie Transformers so I computed"
        },
        {
            "time": 355.84,
            "text": "these Yellow Boxes the way RNN does it"
        },
        {
            "time": 359.0,
            "text": "then I reverse the sentence so I feed in"
        },
        {
            "time": 360.96,
            "text": "Transformers movie is the enjoyed I and"
        },
        {
            "time": 364.199,
            "text": "I again do the computation and then"
        },
        {
            "time": 365.8,
            "text": "again compute some other say green boxes"
        },
        {
            "time": 368.639,
            "text": "right and now I have two representations"
        },
        {
            "time": 371.759,
            "text": "for every word say movie I have one"
        },
        {
            "time": 374.08,
            "text": "representation computed from the forward"
        },
        {
            "time": 375.8,
            "text": "Direction and another representation"
        },
        {
            "time": 377.639,
            "text": "computed from the backward Direction and"
        },
        {
            "time": 379.52,
            "text": "I could just simply concatenate those"
        },
        {
            "time": 381.479,
            "text": "two representations to get the final"
        },
        {
            "time": 383.0,
            "text": "representation of the word movie right"
        },
        {
            "time": 385.24,
            "text": "and the reason I'm doing this or reason"
        },
        {
            "time": 387.24,
            "text": "I'm Computing this one step at a time"
        },
        {
            "time": 389.919,
            "text": "is because I am interested in the"
        },
        {
            "time": 391.84,
            "text": "context of the sentence right I want a"
        },
        {
            "time": 393.88,
            "text": "contextual representation for the word"
        },
        {
            "time": 395.96,
            "text": "movie so that's why I was doing this one"
        },
        {
            "time": 397.84,
            "text": "step at a time to get the contextual"
        },
        {
            "time": 399.639,
            "text": "representations for every word in the"
        },
        {
            "time": 401.919,
            "text": "input right so which is also aware about"
        },
        {
            "time": 403.68,
            "text": "what is happening in the words around it"
        },
        {
            "time": 406.0,
            "text": "so this is important right so the"
        },
        {
            "time": 407.4,
            "text": "contextual representation is important"
        },
        {
            "time": 409.599,
            "text": "what is not good is in the interest of"
        },
        {
            "time": 411.639,
            "text": "doing this contextual computation I'm"
        },
        {
            "time": 414.12,
            "text": "not being able to do a parallel"
        },
        {
            "time": 415.8,
            "text": "processing I have to wait every time one"
        },
        {
            "time": 417.96,
            "text": "word at a time which is"
        },
        {
            "time": 420.12,
            "text": "significantly uh which reduces my"
        },
        {
            "time": 422.24,
            "text": "computational efficiency right because"
        },
        {
            "time": 423.639,
            "text": "I'm just doing things sequentially right"
        },
        {
            "time": 426.24,
            "text": "so now my wish list would be to be able"
        },
        {
            "time": 428.599,
            "text": "to do this to get the contextual"
        },
        {
            "time": 430.479,
            "text": "representation that means when I'm"
        },
        {
            "time": 431.759,
            "text": "Computing the yellow box for the word"
        },
        {
            "time": 434.08,
            "text": "movie I want to know what is happening"
        },
        {
            "time": 436.16,
            "text": "around me I want to take inputs from the"
        },
        {
            "time": 437.8,
            "text": "other words and right now these inputs"
        },
        {
            "time": 439.759,
            "text": "are flowing through these hidden"
        },
        {
            "time": 440.96,
            "text": "representations at 0 H1 H2 H3 so I want"
        },
        {
            "time": 443.879,
            "text": "that to continue right and of course I'm"
        },
        {
            "time": 445.759,
            "text": "doing bidirectional so it's flowing from"
        },
        {
            "time": 447.36,
            "text": "both sides so I want that to happen I"
        },
        {
            "time": 449.12,
            "text": "want to take the inputs from all the"
        },
        {
            "time": 450.639,
            "text": "surrounding words but I don't want to do"
        },
        {
            "time": 452.599,
            "text": "this in a sequential manner I want a"
        },
        {
            "time": 454.12,
            "text": "model which is not recurrent in nature"
        },
        {
            "time": 456.44,
            "text": "because the idea behind a recurrent"
        },
        {
            "time": 458.52,
            "text": "equation is that you do something at"
        },
        {
            "time": 460.479,
            "text": "time step T minus one and then feed it"
        },
        {
            "time": 462.16,
            "text": "as input to time step T so I don't want"
        },
        {
            "time": 463.96,
            "text": "that to happen right so that's the uh"
        },
        {
            "time": 465.96,
            "text": "basic problem I have with these sequence"
        },
        {
            "time": 468.4,
            "text": "to sequence models which I would like to"
        },
        {
            "time": 470.36,
            "text": "overcome right and problem as well as a"
        },
        {
            "time": 472.28,
            "text": "good thing a good thing is that I want"
        },
        {
            "time": 473.479,
            "text": "contextual representations the bad thing"
        },
        {
            "time": 475.12,
            "text": "is I don't want to do parallel uh"
        },
        {
            "time": 476.72,
            "text": "sequential process right and once this"
        },
        {
            "time": 478.879,
            "text": "is done then you have the entire encod"
        },
        {
            "time": 480.96,
            "text": "uh uh decoder block right which then"
        },
        {
            "time": 483.159,
            "text": "takes in the final representation so"
        },
        {
            "time": 485.52,
            "text": "ignore this for some reason this S2 S1"
        },
        {
            "time": 487.599,
            "text": "is showing up on the slide it's not"
        },
        {
            "time": 489.12,
            "text": "supposed to but it continues to show up"
        },
        {
            "time": 491.479,
            "text": "so this is the uh final State and then"
        },
        {
            "time": 493.479,
            "text": "it's passed to the decoder and then the"
        },
        {
            "time": 495.319,
            "text": "decoder again does this uh sequential"
        },
        {
            "time": 497.319,
            "text": "processing right and there of course I"
        },
        {
            "time": 498.919,
            "text": "don't have much of a choice because I"
        },
        {
            "time": 500.96,
            "text": "produce the first output which say in"
        },
        {
            "time": 503.039,
            "text": "this case is Nan and then that has to be"
        },
        {
            "time": 505.919,
            "text": "fed to the next state anyways right so"
        },
        {
            "time": 507.72,
            "text": "the decoder uh this processing will"
        },
        {
            "time": 510.28,
            "text": "still happen sequentially because I need"
        },
        {
            "time": 512.64,
            "text": "to know what was produced and then feed"
        },
        {
            "time": 514.24,
            "text": "it as the next input so unlike here"
        },
        {
            "time": 516.519,
            "text": "where the entire input was available at"
        },
        {
            "time": 518.24,
            "text": "one go here the input itself right is"
        },
        {
            "time": 520.68,
            "text": "being generated one step at a time so"
        },
        {
            "time": 522.839,
            "text": "I'll have to do uh sequential processing"
        },
        {
            "time": 525.0,
            "text": "right but in the encoder can I do"
        },
        {
            "time": 526.519,
            "text": "something to speed up the computation is"
        },
        {
            "time": 528.72,
            "text": "what my uh question is right so we'll go"
        },
        {
            "time": 531.519,
            "text": "towards answering that question okay so"
        },
        {
            "time": 534.519,
            "text": "this is for the decoder where I'm doing"
        },
        {
            "time": 536.12,
            "text": "one word at a time okay but in the"
        },
        {
            "time": 539.24,
            "text": "traditional encoder decoder model the"
        },
        {
            "time": 541.2,
            "text": "problem is that I just do this"
        },
        {
            "time": 543.12,
            "text": "computation once I have computed this uh"
        },
        {
            "time": 545.76,
            "text": "H1 to H5 and then I take H5 as the like"
        },
        {
            "time": 548.959,
            "text": "my final representation for the entire"
        },
        {
            "time": 550.76,
            "text": "sentence and then this is the only thing"
        },
        {
            "time": 552.76,
            "text": "which is fed to the uh decoder and then"
        },
        {
            "time": 555.72,
            "text": "the decoder just produces the entire"
        },
        {
            "time": 557.48,
            "text": "output based on this one representation"
        },
        {
            "time": 559.72,
            "text": "that was given to it there's no notion"
        },
        {
            "time": 561.48,
            "text": "of alignment right so there's no notion"
        },
        {
            "time": 563.279,
            "text": "that when I'm producing Nan I should"
        },
        {
            "time": 565.36,
            "text": "actually focus more on I when I'm"
        },
        {
            "time": 567.8,
            "text": "producing U uh Transformer I should"
        },
        {
            "time": 570.519,
            "text": "actually play more attention to"
        },
        {
            "time": 571.88,
            "text": "Transformer and so on right so that"
        },
        {
            "time": 573.839,
            "text": "notion is not there and you know where"
        },
        {
            "time": 575.76,
            "text": "that notion comes from or what kind of"
        },
        {
            "time": 577.36,
            "text": "models have that notion and that is the"
        },
        {
            "time": 579.8,
            "text": "attention based models right so in the"
        },
        {
            "time": 581.2,
            "text": "attention based models I have that where"
        },
        {
            "time": 583.839,
            "text": "I have I compute the RNN uh encodings"
        },
        {
            "time": 587.519,
            "text": "right and I'm just going to look at it a"
        },
        {
            "time": 589.24,
            "text": "bit differently right so once I have"
        },
        {
            "time": 590.68,
            "text": "computed this I once I've computed H1 to"
        },
        {
            "time": 593.04,
            "text": "H5 now I don't need the RNN block I just"
        },
        {
            "time": 596.56,
            "text": "need these yellow representations that I"
        },
        {
            "time": 598.68,
            "text": "have computed uted which are the H1 to"
        },
        {
            "time": 600.88,
            "text": "H5 right so I can just take them out and"
        },
        {
            "time": 603.32,
            "text": "those can be my uh those are now with me"
        },
        {
            "time": 606.04,
            "text": "right so I don't need to do any further"
        },
        {
            "time": 607.64,
            "text": "computation on these right and once"
        },
        {
            "time": 609.92,
            "text": "these five blocks are available to me"
        },
        {
            "time": 611.519,
            "text": "right so I've just made a copy of those"
        },
        {
            "time": 613.76,
            "text": "representations and kept with me so the"
        },
        {
            "time": 616.36,
            "text": "uh Network seems to be so once all these"
        },
        {
            "time": 618.6,
            "text": "vectors are available I can just throw"
        },
        {
            "time": 620.079,
            "text": "away the encoder and just have the"
        },
        {
            "time": 621.959,
            "text": "output of the encoder which is these"
        },
        {
            "time": 623.44,
            "text": "five vectors in this case in general it"
        },
        {
            "time": 625.72,
            "text": "would be capital T vectors where T is"
        },
        {
            "time": 628.92,
            "text": "the length of my input sequence right so"
        },
        {
            "time": 631.32,
            "text": "that's what I'll have now once I have"
        },
        {
            "time": 633.519,
            "text": "these the I'll feed these as input to"
        },
        {
            "time": 636.079,
            "text": "the attention mechanism right and this"
        },
        {
            "time": 638.399,
            "text": "is what the attention mechanism does at"
        },
        {
            "time": 640.12,
            "text": "every point it now feeds a contextual"
        },
        {
            "time": 643.12,
            "text": "representation to the decoder so it will"
        },
        {
            "time": 645.2,
            "text": "just compute uh what is the most"
        },
        {
            "time": 647.36,
            "text": "important word at this point right so"
        },
        {
            "time": 649.079,
            "text": "you start okay go start Computing the"
        },
        {
            "time": 651.56,
            "text": "output or start building the output so"
        },
        {
            "time": 653.92,
            "text": "it will just take a weighted"
        },
        {
            "time": 655.0,
            "text": "representation of all the inputs to"
        },
        {
            "time": 656.76,
            "text": "compute the contextual representation"
        },
        {
            "time": 658.68,
            "text": "which is is going to be like a attention"
        },
        {
            "time": 660.56,
            "text": "weighted uh sum of the inputs right so"
        },
        {
            "time": 663.519,
            "text": "that's what it's going to be so I have"
        },
        {
            "time": 666.079,
            "text": "these Alphas coming in here sorry for"
        },
        {
            "time": 668.72,
            "text": "some reason the animations are showing"
        },
        {
            "time": 670.2,
            "text": "up very slowly yeah so I have these"
        },
        {
            "time": 672.0,
            "text": "Alphas showing up here and then I take a"
        },
        {
            "time": 674.76,
            "text": "weighted some and I compute this uh uh"
        },
        {
            "time": 677.639,
            "text": "contextual representation uh C1 right so"
        },
        {
            "time": 680.399,
            "text": "this is called the context Vector it's"
        },
        {
            "time": 682.12,
            "text": "also called The Thought Vector uh for"
        },
        {
            "time": 684.48,
            "text": "the rest of the discussion I'll"
        },
        {
            "time": 685.76,
            "text": "typically call it the context Vector if"
        },
        {
            "time": 687.48,
            "text": "I'm calling it something else I'll let"
        },
        {
            "time": 689.16,
            "text": "you know uh at that point right so just"
        },
        {
            "time": 691.68,
            "text": "think of the C1 as a context Vector"
        },
        {
            "time": 693.519,
            "text": "which is a weighted sum or the attention"
        },
        {
            "time": 695.48,
            "text": "weighted sum of the inputs right so you"
        },
        {
            "time": 697.079,
            "text": "have just taken the outputs of the"
        },
        {
            "time": 698.399,
            "text": "encoder uh which were these yellow"
        },
        {
            "time": 700.399,
            "text": "representations which are context aware"
        },
        {
            "time": 702.48,
            "text": "representations and now again to the"
        },
        {
            "time": 704.36,
            "text": "decoder you have feeding a contextual"
        },
        {
            "time": 706.079,
            "text": "representation which is now here the"
        },
        {
            "time": 708.12,
            "text": "context is basically where am I on the"
        },
        {
            "time": 710.0,
            "text": "output I'm producing the first word so"
        },
        {
            "time": 712.24,
            "text": "what is the most important uh set of"
        },
        {
            "time": 714.36,
            "text": "weights or what is the most important"
        },
        {
            "time": 715.68,
            "text": "words that I need to focus on right and"
        },
        {
            "time": 717.519,
            "text": "then you keep doing this at every every"
        },
        {
            "time": 719.44,
            "text": "uh time step so you could think of is"
        },
        {
            "time": 721.36,
            "text": "that you had this H1 to H5 weights and"
        },
        {
            "time": 724.04,
            "text": "then you multiplying them uh sorry the"
        },
        {
            "time": 726.44,
            "text": "H1 to H5 uh vectors right so you can"
        },
        {
            "time": 729.76,
            "text": "think of putting them in a matrix and"
        },
        {
            "time": 732.2,
            "text": "then you have this Vector of Weights so"
        },
        {
            "time": 734.12,
            "text": "now you're taking the doing this Matrix"
        },
        {
            "time": 736.44,
            "text": "Vector multiplication which is"
        },
        {
            "time": 738.32,
            "text": "essentially taking a linear combination"
        },
        {
            "time": 740.88,
            "text": "of all these columns right so Alpha 1"
        },
        {
            "time": 743.04,
            "text": "one into this Alpha 1 2 into this Alpha"
        },
        {
            "time": 745.0,
            "text": "13 into this and so on right so that's"
        },
        {
            "time": 746.6,
            "text": "the operation that is happening here and"
        },
        {
            "time": 749.639,
            "text": "then you get the uh you feed it to the"
        },
        {
            "time": 751.959,
            "text": "decoder RNN which then produces a output"
        },
        {
            "time": 755.04,
            "text": "at the end right and you keep repeating"
        },
        {
            "time": 757.36,
            "text": "this at every time step you do it at C2"
        },
        {
            "time": 759.68,
            "text": "then you compute C3 C4 C5 and so on"
        },
        {
            "time": 762.76,
            "text": "right so you keep uh doing that now what"
        },
        {
            "time": 765.48,
            "text": "I'm showing here is what is known as a"
        },
        {
            "time": 767.56,
            "text": "heat map so this is what you typically"
        },
        {
            "time": 769.88,
            "text": "look at when you are using an attention"
        },
        {
            "time": 772.199,
            "text": "based model so this has uh this is a say"
        },
        {
            "time": 777.12,
            "text": "a"
        },
        {
            "time": 777.839,
            "text": "T1"
        },
        {
            "time": 781.32,
            "text": "cross T2 Matrix where T1 is the length"
        },
        {
            "time": 784.32,
            "text": "of the input and T2 is the length of the"
        },
        {
            "time": 787.56,
            "text": "output uh uh so or the other way around"
        },
        {
            "time": 791.44,
            "text": "whichever way you can look at it and now"
        },
        {
            "time": 793.36,
            "text": "in this wherever you see a light spot"
        },
        {
            "time": 796.399,
            "text": "that is the place where the attention"
        },
        {
            "time": 798.16,
            "text": "weight is maximum right so when I was"
        },
        {
            "time": 800.24,
            "text": "generating I my attention on Nan was"
        },
        {
            "time": 802.959,
            "text": "maximum when I was generating enjoyed my"
        },
        {
            "time": 805.68,
            "text": "attention on uh riane was maximum when I"
        },
        {
            "time": 809.24,
            "text": "generating fill this was the maximum"
        },
        {
            "time": 811.639,
            "text": "weighted word and similarly here right"
        },
        {
            "time": 813.44,
            "text": "it makes sense because these are almost"
        },
        {
            "time": 815.0,
            "text": "like one to one correspondences in the"
        },
        {
            "time": 817.279,
            "text": "uh translation output right so that's"
        },
        {
            "time": 819.199,
            "text": "what the heat map uh shows you yeah so"
        },
        {
            "time": 822.199,
            "text": "this is how the heat map relates to what"
        },
        {
            "time": 824.639,
            "text": "is happening in the sentence as I said"
        },
        {
            "time": 826.399,
            "text": "that when I'm using Nan uh the maximum"
        },
        {
            "time": 829.199,
            "text": "attention is on I and the color coded"
        },
        {
            "time": 831.399,
            "text": "you can understand the colors here so"
        },
        {
            "time": 833.079,
            "text": "this blue color I'm focusing here and"
        },
        {
            "time": 835.44,
            "text": "this is the corresponding weight and so"
        },
        {
            "time": 836.839,
            "text": "on right and you have some uh attention"
        },
        {
            "time": 839.44,
            "text": "function right and we had seen this"
        },
        {
            "time": 841.44,
            "text": "function so used to compute the"
        },
        {
            "time": 844.16,
            "text": "attention weight right as some function"
        },
        {
            "time": 847.56,
            "text": "of the previous state of the decoder and"
        },
        {
            "time": 850.32,
            "text": "the uh input vectors that you had so"
        },
        {
            "time": 853.199,
            "text": "this was the attention to be paid to"
        },
        {
            "time": 856.12,
            "text": "input I at time step T which depended on"
        },
        {
            "time": 858.959,
            "text": "the state of the decoder at time step T"
        },
        {
            "time": 860.8,
            "text": "minus one and uh input I right and then"
        },
        {
            "time": 864.48,
            "text": "just had this softmax function to make"
        },
        {
            "time": 866.8,
            "text": "sure that this uh align this Alphas"
        },
        {
            "time": 869.639,
            "text": "formed the distribution right so they"
        },
        {
            "time": 871.16,
            "text": "summed up to one right so that's what we"
        },
        {
            "time": 873.399,
            "text": "have seen and you can use any alignment"
        },
        {
            "time": 875.639,
            "text": "function here uh we had seen one"
        },
        {
            "time": 877.639,
            "text": "specific uh function in when we are"
        },
        {
            "time": 880.0,
            "text": "discussing recurrent neural networks so"
        },
        {
            "time": 882.12,
            "text": "just showing the alignment uh function"
        },
        {
            "time": 884.12,
            "text": "again right and here's a question now"
        },
        {
            "time": 885.959,
            "text": "right so and this is what will lead us"
        },
        {
            "time": 887.68,
            "text": "to our eventual discussion on"
        },
        {
            "time": 889.8,
            "text": "Transformers right so can Alpha T be"
        },
        {
            "time": 893.6,
            "text": "computed in parall for all I at time"
        },
        {
            "time": 896.199,
            "text": "step T so what is the question that I'm"
        },
        {
            "time": 897.72,
            "text": "asking so I'm at a particular time step"
        },
        {
            "time": 899.519,
            "text": "say I'm at time step 4 right so I'm"
        },
        {
            "time": 902.199,
            "text": "asking can all these Alphas Alpha TI and"
        },
        {
            "time": 905.839,
            "text": "T is equal to 4 so I'm asking whether"
        },
        {
            "time": 907.48,
            "text": "Alpha 41 Alpha 42 43 4 4 4 five because"
        },
        {
            "time": 911.16,
            "text": "I can take values from 1 to five can"
        },
        {
            "time": 913.199,
            "text": "they be computed in parallel right and"
        },
        {
            "time": 916.16,
            "text": "the answer is yes right because this"
        },
        {
            "time": 918.6,
            "text": "only depends on St minus one which is"
        },
        {
            "time": 921.72,
            "text": "already available and on hi which is"
        },
        {
            "time": 924.079,
            "text": "already available right so it depends"
        },
        {
            "time": 925.6,
            "text": "only on these two values so you can"
        },
        {
            "time": 927.519,
            "text": "compute this this in parallel right and"
        },
        {
            "time": 930.399,
            "text": "of course for normalization you need all"
        },
        {
            "time": 931.88,
            "text": "the values but you can compute the"
        },
        {
            "time": 934.319,
            "text": "scores in parallel and then once you"
        },
        {
            "time": 936.0,
            "text": "have the scores you can again compute"
        },
        {
            "time": 937.279,
            "text": "the normalization in parall right so for"
        },
        {
            "time": 939.56,
            "text": "a given T you can compute the alpha TI"
        },
        {
            "time": 943.88,
            "text": "in parallel for All Eyes no matter how"
        },
        {
            "time": 945.88,
            "text": "many eyes you have right so no matter"
        },
        {
            "time": 947.24,
            "text": "how long your sequence is you don't need"
        },
        {
            "time": 949.68,
            "text": "to wait on the previous computation to"
        },
        {
            "time": 952.0,
            "text": "happen or to have something happen at"
        },
        {
            "time": 954.279,
            "text": "IUS one to be able to compute Alpha TI"
        },
        {
            "time": 956.759,
            "text": "right you can just compute all of that"
        },
        {
            "time": 958.04,
            "text": "in Val"
        },
        {
            "time": 959.16,
            "text": "right now the other question is so the"
        },
        {
            "time": 962.279,
            "text": "main takeaway here is that the attention"
        },
        {
            "time": 963.639,
            "text": "can be paralyzed but now the other"
        },
        {
            "time": 964.92,
            "text": "question that I have is that can you"
        },
        {
            "time": 967.079,
            "text": "compute this in parallel for all T's"
        },
        {
            "time": 970.24,
            "text": "right so I said that at a particular T"
        },
        {
            "time": 971.959,
            "text": "you can compute it in parallel right now"
        },
        {
            "time": 973.839,
            "text": "I'm asking that suppose there's this"
        },
        {
            "time": 975.959,
            "text": "time step five also here so can I"
        },
        {
            "time": 978.399,
            "text": "compute Alpha"
        },
        {
            "time": 981.72,
            "text": "4 I right all the alpha fours and all"
        },
        {
            "time": 985.44,
            "text": "the alpha"
        },
        {
            "time": 986.519,
            "text": "fs and all the alpha 3s right so all"
        },
        {
            "time": 990.36,
            "text": "these Alphas across different T's right"
        },
        {
            "time": 993.16,
            "text": "so my T is changing here can I compute"
        },
        {
            "time": 996.199,
            "text": "them in parallel a given set of values"
        },
        {
            "time": 999.44,
            "text": "all the Alpha Four Stars right Alpha 41"
        },
        {
            "time": 1001.68,
            "text": "up to Alpha 4D I can compute in parall"
        },
        {
            "time": 1003.44,
            "text": "that we have already seen but can I"
        },
        {
            "time": 1004.88,
            "text": "compute all of these in parallel at one"
        },
        {
            "time": 1007.079,
            "text": "go and the answer is clearly no right"
        },
        {
            "time": 1009.6,
            "text": "the reason is that it depends on St"
        },
        {
            "time": 1012.759,
            "text": "minus one right so unless I have"
        },
        {
            "time": 1014.48,
            "text": "computed St minus one I cannot compute"
        },
        {
            "time": 1017.0,
            "text": "any of the alpha T's right right and St"
        },
        {
            "time": 1019.36,
            "text": "minus one actually depends on Alpha T"
        },
        {
            "time": 1021.279,
            "text": "minus one because that's the input right"
        },
        {
            "time": 1023.44,
            "text": "so St minus one here this"
        },
        {
            "time": 1027.48,
            "text": "one would depend on"
        },
        {
            "time": 1030.439,
            "text": "uh uh sorry this would depend on C uh"
        },
        {
            "time": 1034.559,
            "text": "three and C3 in turn would be a function"
        },
        {
            "time": 1037.919,
            "text": "of alpha 3 all the alpha 3s right so"
        },
        {
            "time": 1042.559,
            "text": "unless I have computed Alpha 3 I cannot"
        },
        {
            "time": 1044.4,
            "text": "compute C3 unless I have computed C3 I"
        },
        {
            "time": 1047.039,
            "text": "cannot compute S3 unless I computed S3 I"
        },
        {
            "time": 1049.88,
            "text": "cannot compute the alpha Force right so"
        },
        {
            "time": 1051.88,
            "text": "all the alphas cannot be computed in"
        },
        {
            "time": 1053.6,
            "text": "parallel but for a given uh T the alphas"
        },
        {
            "time": 1057.2,
            "text": "can be computed in parall right so now"
        },
        {
            "time": 1060.32,
            "text": "the two things to notice here right so"
        },
        {
            "time": 1061.88,
            "text": "one is that the attention can be"
        },
        {
            "time": 1063.799,
            "text": "parallel at least at a given T it can be"
        },
        {
            "time": 1065.4,
            "text": "parallelized right it cannot be"
        },
        {
            "time": 1067.2,
            "text": "parallelized across T's but for a given"
        },
        {
            "time": 1069.0,
            "text": "T it can be parallelized right and this"
        },
        {
            "time": 1070.64,
            "text": "is something that you would like to"
        },
        {
            "time": 1072.799,
            "text": "exploit and see that if you can get rid"
        },
        {
            "time": 1075.76,
            "text": "of this recurrent connection right"
        },
        {
            "time": 1077.6,
            "text": "because this recurrent connection is"
        },
        {
            "time": 1078.88,
            "text": "still a problem for us right because"
        },
        {
            "time": 1081.32,
            "text": "because of the recurrent connection we"
        },
        {
            "time": 1082.72,
            "text": "have to do things sequentially but if we"
        },
        {
            "time": 1085.28,
            "text": "could get rid of the recurrent"
        },
        {
            "time": 1086.84,
            "text": "connections and then rely on the fact"
        },
        {
            "time": 1088.88,
            "text": "that the alphas can still be computed in"
        },
        {
            "time": 1090.919,
            "text": "parallel then can we get to an"
        },
        {
            "time": 1092.72,
            "text": "architecture which allows us to compute"
        },
        {
            "time": 1094.96,
            "text": "these Alphas in parall right so it's"
        },
        {
            "time": 1097.72,
            "text": "still a bit hard to visualize where we"
        },
        {
            "time": 1099.36,
            "text": "are headed but just keep these questions"
        },
        {
            "time": 1101.48,
            "text": "in mind along the way and once we read"
        },
        {
            "time": 1103.679,
            "text": "there all these questions and the"
        },
        {
            "time": 1104.88,
            "text": "answers will make sense so just to"
        },
        {
            "time": 1106.799,
            "text": "summarize the discussion so far right"
        },
        {
            "time": 1108.44,
            "text": "right so I mean everything about the RNN"
        },
        {
            "time": 1110.36,
            "text": "model is good right so what do I mean by"
        },
        {
            "time": 1112.48,
            "text": "that"
        },
        {
            "time": 1114.76,
            "text": "uh we saw that across papers right I"
        },
        {
            "time": 1117.88,
            "text": "mean we saw the architectures they used"
        },
        {
            "time": 1119.84,
            "text": "for machine translation summarization"
        },
        {
            "time": 1121.6,
            "text": "video captioning image captioning right"
        },
        {
            "time": 1123.36,
            "text": "so they gave very good performance and a"
        },
        {
            "time": 1125.12,
            "text": "wide variety of tasks right but the only"
        },
        {
            "time": 1127.72,
            "text": "uh issue that we have is that given a"
        },
        {
            "time": 1129.919,
            "text": "training example we cannot paralyze the"
        },
        {
            "time": 1132.12,
            "text": "sequence of computations because each of"
        },
        {
            "time": 1134.159,
            "text": "these guys needs to be computed one at a"
        },
        {
            "time": 1136.799,
            "text": "time right I cannot compute all of them"
        },
        {
            "time": 1138.36,
            "text": "in parallel of course on top of that if"
        },
        {
            "time": 1140.039,
            "text": "I have attention attention at a given"
        },
        {
            "time": 1142.039,
            "text": "time step can be computed in parallel"
        },
        {
            "time": 1143.76,
            "text": "right so now our wish list would be can"
        },
        {
            "time": 1146.32,
            "text": "we come up with a new architecture right"
        },
        {
            "time": 1148.919,
            "text": "that incorporates the attention"
        },
        {
            "time": 1150.44,
            "text": "mechanism and also allows us to do"
        },
        {
            "time": 1152.44,
            "text": "things in parallel right so we don't"
        },
        {
            "time": 1153.919,
            "text": "want to get rid of the attention"
        },
        {
            "time": 1155.159,
            "text": "mechanism because the attention"
        },
        {
            "time": 1156.24,
            "text": "mechanism helps us to compute the"
        },
        {
            "time": 1157.72,
            "text": "contextual representation but we want"
        },
        {
            "time": 1159.64,
            "text": "parallelism we also don't have a problem"
        },
        {
            "time": 1161.76,
            "text": "with the basic idea of recurrence right"
        },
        {
            "time": 1164.24,
            "text": "that the recurrence actually allows us"
        },
        {
            "time": 1166.44,
            "text": "to uh compute things"
        },
        {
            "time": 1168.799,
            "text": "which are contextual right so we don't"
        },
        {
            "time": 1170.36,
            "text": "have a problem with this here so we"
        },
        {
            "time": 1172.84,
            "text": "don't have a problem with this here this"
        },
        {
            "time": 1175.039,
            "text": "is fine because this is allowing us to"
        },
        {
            "time": 1176.84,
            "text": "compute these recurrent uh or the"
        },
        {
            "time": 1179.039,
            "text": "contextual representations but we have a"
        },
        {
            "time": 1181.0,
            "text": "problem with the computational costs"
        },
        {
            "time": 1182.96,
            "text": "which comes with recurrent that's the"
        },
        {
            "time": 1184.36,
            "text": "problem that we want to solve okay so"
        },
        {
            "time": 1186.48,
            "text": "that's the context so that's a quick"
        },
        {
            "time": 1188.32,
            "text": "recap of recurrent Ural networks and"
        },
        {
            "time": 1190.919,
            "text": "what we see as problems in the recurrent"
        },
        {
            "time": 1192.76,
            "text": "neural network and now we'll try to go"
        },
        {
            "time": 1194.72,
            "text": "towards a solution for that right and"
        },
        {
            "time": 1196.6,
            "text": "that might probably lead us to a new"
        },
        {
            "time": 1198.24,
            "text": "arure so I'll end this video here and"
        },
        {
            "time": 1200.44,
            "text": "we'll come back and uh continue from"
        },
        {
            "time": 1202.48,
            "text": "this"
        },
        {
            "time": 1206.36,
            "text": "point"
        }
    ],
    "date": "2024-12-21T17:39:24.688954"
}